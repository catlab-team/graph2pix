<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8">
    <meta name="description"
          content="##########">
    <meta name="keywords" content="LatentCLR">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Graph2Pix: A Graph-Based Image to Image Translation Framework</title>
    <!-- BAAk -->
    <!-- Global site tag (gtag.js) - Google Analytics  -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
    <script>
        window.dataLayer = window.dataLayer || [];

        function gtag() {
            dataLayer.push(arguments);
        }

        gtag('js', new Date());

        gtag('config', 'G-PYVRSFMDRL');


    </script>

    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
          rel="stylesheet">

    <link rel="stylesheet" href="./static/css/bulma.min.css">
    <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
    <link rel="stylesheet"
          href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="./static/css/index.css">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script defer src="./static/js/fontawesome.all.min.js"></script>
    <script src="./static/js/bulma-carousel.min.js"></script>
    <script src="./static/js/bulma-slider.min.js"></script>
    <script src="./static/js/index.js"></script>
</head>
<body>


<section class="hero">
    <div class="hero-body">
        <div class="container is-max-desktop">
            <div class="columns is-centered">
                <div class="column has-text-centered">
                                     <h3 class="title is-4">In AIM ICCV 2021</h3>

                    <h1 class="title is-1 publication-title">Graph2Pix: A Graph-Based Image to Image Translation Framework</h1>
                    <div class="is-size-5 publication-authors">
            <span class="author-block">
              <span>Dilara Gokay<sup>1*</sup>,</span>
                        <span class="author-block">
              <span>Enis Simsar<sup>1*</sup>,</span>
                        <span class="author-block">
              <span>Efehan Atici<span><sup>2</sup>,
            </span>
        </div>
        <div class="is-size-5 publication-authors">
            <span class="author-block">
                <span>Alper Ahmetoglu<span><sup>2</sup>,
              </span>
              <span class="author-block">
                <span>Atif Emre Yuksel<span><sup>2</sup>,
              </span>
                        <span class="author-block">
              <a href="https://pinguar.org/">Pinar Yanardag</a><sup>2</sup>
            </span>
                    </div>

                    <div class="is-size-5 publication-authors">
                        <span class="author-block"><sup>1</sup>Technical University of Munich,</span>
                        <span class="author-block"><sup>2</sup>Bogazici University</span>
                    </div>

                    <div class="is-size-5 publication-authors">
                        <span class="author-block">
                            <span><small><sup>*</sup>Denotes equal contribution</small></span>
                    </div>

                    <div class="column has-text-centered">
                        <div class="publication-links">
                            <!-- PDF Link. -->

                            <span class="link-block">
                <a href="https://arxiv.org/abs/2108.09752"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
                            <!-- Code Link. -->
                            <span class="link-block">
                <a href="https://github.com/catlab-team/Graph2Pix"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>

                        </div>

                    </div>

                </div>

            </div>

        </div>

    </div>

</section>



<!--/ Paper video. -->

<section class="section">
    <div class="container is-max-desktop">
        <!-- Abstract. -->
        <div class="columns is-centered has-text-centered">
            <div class="column is-four-fifths">
                <h2 class="title is-3">Abstract</h2>
                <div class="content has-text-justified">

                    <p>
                        In this paper, we propose a graph-based image-to-image translation framework for generating images. We use rich data collected from the popular creativity platform Artbreeder (<a href="http://artbreeder.com">http://artbreeder.com</a>), where users interpolate multiple GAN-generated images to create artworks. This unique approach of creating new images leads to a tree-like structure where one can track historical data about the creation of a particular image. Inspired by this structure, we propose a novel graph-to-image translation model called Graph2Pix, which takes a graph and corresponding images as input and generates a single image as output. Our experiments show that Graph2Pix is able to outperform several image-to-image translation frameworks on benchmark metrics, including LPIPS (with a 25% improvement) and human perception studies (n=60), where users preferred the images generated by our method 81.5% of the time.
                    </p>
                </div>

            </div>

        </div>
        <!--/ Abstract. -->    
    </div>
</section>

<section class="section">
    <div class="container is-max-desktop content">
        <h2 class="title">Dataset</h2>
        <img src="static/images/lineage_new.png">
        <p>
            <small>
                A sample lineage data up to 2 levels (Access the full tree via <a href="https://www.artbreeder.com/lineage?k=1fcdf872ec11c80e955bb5c1">https://www.artbreeder.com/lineage?k=1fcdf872ec11c80e955bb5c1</a>.)  The creators of the images are annotated with labels
            </small>
        </p>
        <p>
            One of the most popular GAN-based creativity platforms is Artbreeder. The easy-to-use interface of the platform attracted thousands of users and enabled them to generate over 70 million GAN-based images. ArtBreeder helps users to create new images using BigGAN [4] or StyleGAN based models where users are able to adjust parameters or blend different images to generate new ones. Users can breed new images from a single one by editing genes such as ages, gender, ethnicity or create new ones by crossbreeding multiple images together. The unique ability of the crossbreeding  functionality allows generated images to have lineage data where the ancestors of the generated image can be tracked in a tree-based structure.       
        </p>
        <p>
            The lineage structure of images generated with Artbreeder opens up a wide range of possible applications, but also presents unique challenges. The lineage information provides a tree-like structure in which one can trace the parents, grandparents, and further ancestors of a given image. However, it is not entirely clear how such a  structure can be used in GAN-based models. For example, how can we generate a child image based on a list of its ancestors?  One could try to use image-to-image translation methods such as Pix2Pix or CycleGAN and use any ancestor to generate the child image (e.g.,  feeding Parent<sup>1</sup> and generating Child<sup>1</sup> in Figure. However, this approach results in a significant loss of information since only one ancestor can be used (e.g., Parent<sup>2</sup> or the grandparents are considered).
        </p>
        <p>
            In this paper, we propose a novel image-to-image translation method that takes multiple images and their corresponding lineage structure and generates the target image as output. To propose a general solution, our framework takes a graph structure, considering the tree structure of lineage data as a sub-case. To the best of our knowledge, this is the first image-to-image translation method that uses a graph-based structure.
        </p>
    </div>
</section>

<section class="section">
    <div class="container is-max-desktop content">
        <h2 class="title">Graph2Pix Framework</h2>
        <img src="static/images/final_gcn.png">
        <p>
            An illustration of Graph2Pix. During the generation process (shown on the left) our 2-layer GCN module takes A<sub>1</sub> ... A<sub>n</sub>, the ancestors  of an image as an input, and generates the prediction. The discriminator (shown on the right) takes the concatenation of the input images A<sub>1</sub> ... A<sub>n</sub> and the generated image G(x) and creates a prediction. Note that we placed GCN after the convolutional layer to use it more efficiently.
        </p>
    </div>
</section>

<section class="section">
    <div class="container is-max-desktop content">
        <h2 class="title">Experiments</h2>
        <p>
            We compared our method against several baselines quantitatively and qualitatively. Our quantitative experiments show that our method outperforms the competitors and improves LPIPS, FID, and KID scores by 25%, 0.3%, and 12%, respectively. Furthermore, our qualitative experiments show that human participants prefer the images generated by our method 81.5% of the time compared to its competitors.
        </p>
        <h3 class="title">Qualitative Experiments</h3>
        <img src="static/images/newsamples3.png">
        <p>
            A qualitative comparison of our method with the top performing competitors, Pix2PixHD and U-GAT-IT. First-level ancestors (Parent<sub>1</sub> and Parent<sub>2</sub>) are shown on the left where the ground truth image is denoted with Child. As can be seen from the results, our method is able to incorporate the ancestor information when generating the images whereas Pix2PixHD and U-GAT-IT are limited to a single ancestor image (Parent<sub>1</sub>). Due to lack of space, only first-degree parents are  shown, however a sample lineage for the bottom-left image can be seen on Artbreeder (<a href="https://www.artbreeder.com/lineage?k=f8c31131db9a73ac0425">https://www.artbreeder.com/lineage?k=f8c31131db9a73ac0425</a>).
        </p>
        <h3 class="title">Human Evaluations</h3>
        <img src="static/images/histogram.png">
        <p>
            The results for each question in our human evaluation survey are shown on the left  (the x-axis represents the image IDs, the y-axis represents the number of votes received for each method). The top-performing image, where participants unanimously found our method was most successful, is shown in the upper right. Our method is the least successful on the image shown on the bottom right, where participants preferred the Pix2PixHD result. 
        </p>
    </div>

</section>

<section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
        <h2 class="title">BibTeX</h2>
        <pre><code>@inproceedings{graph2pix2021,
    title={Graph2Pix: A Graph-Based Image to Image Translation Framework},
    author={Gokay, Dilara and Simsar, Enis and Atici, Efehan and Ahmetoglu, Alper and Yuksel, Atif Emre and Yanardag, Pinar},
    booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
    pages={2001--2010},
    year={2021}
}</code></pre>
    </div>
</section>


<section class="section" id="ack">
    <div class="container is-max-desktop content">
        <h2 class="title">Acknowledgments</h2>
        <p>
            This publication has been produced
            benefiting from the 2232 International Fellowship for Outstanding Researchers Program of TUBITAK (Project No:
            118c321). We also acknowledge the support of NVIDIA
            Corporation through the donation of the TITAN X GPU and
            GCP research credits from Google. We also would like to
            thank Joel Simon for their support in collecting the dataset.
        </p>
    </div>
</section>


<footer class="footer">
    <div class="container">

        <div class="columns is-centered">
            <div class="column is-8">
                <div class="content">
                    <p>
                        This website is licensed under a <a rel="license"
                                                            href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
                        Commons Attribution-ShareAlike 4.0 International License</a>.
                    </p>
                    <p>
                        Template from <a
                            href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>
                    </p>
                </div>
            </div>
        </div>
    </div>
</footer>

</body>
</html>
